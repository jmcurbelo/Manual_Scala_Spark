{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando RDDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primera forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stringList: Array[String] = Array(Jose, Aliannys, Idalmi)\n",
       "stringRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at <console>:28\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stringList = Array(\"Jose\",\"Aliannys\",\"Idalmi\")\n",
    "val stringRDD = sc.parallelize(stringList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segunda  forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way to create an RDD is to read a dataset from a storage system, which\n",
    "can be a local computer file system, HDFS, Cassandra, Amazon S3, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileRDD: org.apache.spark.rdd.RDD[String] = test_RDD.txt MapPartitionsRDD[12] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileRDD = sc.textFile(\"test_RDD.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tercera  forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third way to create an RDD is by invoking one of the transformation operations\n",
    "on an existing RDD. Once you start becoming competent with Spark, you will do this all\n",
    "the time without thinking twice about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOSE\n",
      "ALIANNYS\n",
      "IDALMI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "namesUP: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[13] at map at <console>:28\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val namesUP = stringRDD.map(line => line.toUpperCase)\n",
    "namesUP.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra vía que permite el mantenimiento del código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOSE\n",
      "ALIANNYS\n",
      "IDALMI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mayuscula: (line: String)String\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mayuscula(line: String): String = {\n",
    "    line.toUpperCase\n",
    "}\n",
    "\n",
    "stringRDD.map(l => mayuscula(l)).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stringLengthRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[15] at map at <console>:28\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stringLengthRDD = stringRDD.map(l => l.length)\n",
    "stringLengthRDD.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flatMap(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frases: Array[String] = Array(Jose lindo, Tuyo y mío, solo tu y yo)\n",
       "frasesRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[16] at parallelize at <console>:28\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val frases = Array(\"Jose lindo\",\"Tuyo y mío\", \"solo tu y yo\")\n",
    "val frasesRDD = sc.parallelize(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jose\n",
      "lindo\n",
      "Tuyo\n",
      "y\n",
      "mío\n",
      "solo\n",
      "tu\n",
      "y\n",
      "yo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at flatMap at <console>:26\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordRDD = frasesRDD.flatMap(l => l.split(\" \"))\n",
    "wordRDD.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jose\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joseRDD: Unit = ()\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joseRDD = stringRDD.filter(line => line.contains(\"Jose\")).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapPartitions(func)/mapPartitionsWithIndex(index, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the mapPartitions and mapPartitionsWithIndex transformations are used\n",
    "to optimize the performance of your data processing logic by reducing the number of\n",
    "times the expensive setup step is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example first creates an RDD with two partitions and then creates a random\n",
    "generator per partitions before iterating through each row. Finally, as it iterates through\n",
    "the row, it adds a random number to each row in each partition in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\n",
       "sampleList: Array[String] = Array(One, Two, Three, Four, Five)\n",
       "sampleRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[19] at parallelize at <console>:28\n",
       "result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at mapPartitions at <console>:30\n",
       "res13: Array[String] = Array(One:-1620372493, Two:856580103, Three:1741214079, Four:1089910951, Five:1496680526)\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "val sampleList = Array(\"One\", \"Two\", \"Three\", \"Four\",\"Five\")\n",
    "\n",
    "val sampleRDD = spark.sparkContext.parallelize(sampleList, 2)\n",
    "\n",
    "val result = sampleRDD.mapPartitions((itr:Iterator[String]) => {\n",
    "    val rand = new Random(System.currentTimeMillis + Random.nextInt)\n",
    "    itr.map(l => l + \":\" + rand.nextInt)\n",
    "    })\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente crea un objeto de tipo RANDOM y lo llama cada vez que se desee para generar números aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand: scala.util.Random = scala.util.Random@32d907ad\n",
       "res18: Int = 1939282200\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rand = new Random(System.currentTimeMillis + Random.nextInt)\n",
    "rand.nextInt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Function to Encapsulate the Logic of Adding Random Numbers to Each Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\n",
       "addRandomNumber: (rows: Iterator[String])Iterator[String]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "def addRandomNumber(rows:Iterator[String]) = {\n",
    "    val rand = new Random(System.currentTimeMillis + Random.nextInt)\n",
    "    rows.map(l => l + \" : \" + rand.nextInt)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the addRandomNumber Function in the mapPartitions\n",
    "Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[21] at mapPartitions at <console>:30\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = sampleRDD.mapPartitions((rows:Iterator[String]) => addRandomNumber(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mapPartitionsWithIndex Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numberRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[22] at parallelize at <console>:26\n",
       "res19: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (0,4), (0,5), (1,6), (1,7), (1,8), (1,9), (1,10))\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numberRDD = spark.sparkContext.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)\n",
    "numberRDD.mapPartitionsWithIndex((idx:Int, itr:Iterator[Int]) => {\n",
    "    itr.map(n => (idx, n) )}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones de conjunto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### union(otherRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[24] at parallelize at <console>:27\n",
       "rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[25] at parallelize at <console>:28\n",
       "rdd3: org.apache.spark.rdd.RDD[Int] = UnionRDD[26] at union at <console>:29\n",
       "res21: Array[Int] = Array(1, 2, 3, 4, 5, 1, 6, 7, 8)\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Array(1,2,3,4,5))\n",
    "val rdd2 = sc.parallelize(Array(1,6,7,8))\n",
    "val rdd3 = rdd1.union(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### intersection(otherRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Any] = ParallelCollectionRDD[28] at parallelize at <console>:32\n",
       "rdd2: org.apache.spark.rdd.RDD[Any] = ParallelCollectionRDD[29] at parallelize at <console>:33\n",
       "rdd3: org.apache.spark.rdd.RDD[Any] = MapPartitionsRDD[35] at intersection at <console>:34\n",
       "res22: Array[Any] = Array(jose)\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Array(1,\"jose\"))\n",
    "val rdd2 = sc.parallelize(Array(2,4,6,\"jose\"))\n",
    "val rdd3 = rdd1.intersection(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### substract(otherRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Removing Stop Words Using the subtract Transformation/no funciona substract!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "35: error: value substract is not a member of org.apache.spark.rdd.RDD[String]",
     "output_type": "error",
     "traceback": [
      "<console>:35: error: value substract is not a member of org.apache.spark.rdd.RDD[String]",
      "       val realWords = words.substract(stopWords)",
      "                             ^",
      ""
     ]
    }
   ],
   "source": [
    "val words = spark.sparkContext.parallelize(List(\"The amazing thing about spark is that it is very simple to learn\"))\n",
    ".flatMap(l => l.split(\" \"))\n",
    ".map(w => w.toLowerCase)\n",
    "\n",
    "val stopWords = spark.sparkContext.parallelize(List(\"the it is to that\"))\n",
    ".flatMap(l => l.split(\" \"))\n",
    "\n",
    "val realWords = words.substract(stopWords)\n",
    "realWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distinct( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Any] = ParallelCollectionRDD[44] at parallelize at <console>:29\n",
       "res25: Array[Any] = Array(dos, tres, 1, uno, 2, 3)\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(1,\"uno\",\"uno\",2,3,3,2,\"dos\",\"tres\"))\n",
    "rdd1.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample(withReplacement, fraction, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The withReplacement parameter determines whether an already sampled row will be placed back into RDD for the next sampling\n",
    "* The given fraction value must be between 0 and 1, and it is not guaranteed that the returned RDD will have the exact fraction number of rows of the original RDD\n",
    "* The optional seed parameter is used to seed the random generator, and it has a default value if one is not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[48] at parallelize at <console>:29\n",
       "res26: Array[Int] = Array(4, 10, 10)\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))\n",
    "rdd1.sample(true, 0.4).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It collects all the rows from each of the partitions in an RDD and brings them over to the driver program. If your RDD contains 100 million rows, then it is not a good idea to invoke the collect action because the driver program most likely doesn’t have sufficient memory to hold all those rows. As a result, the driver will most likely run into an out-of-memory error and your Spark application or shell will die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[50] at parallelize at <console>:29\n",
       "res29: Long = 10\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8,9,10), 2)\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[51] at parallelize at <console>:29\n",
       "res30: Int = 1\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(1,2,3,4,5))\n",
    "rdd1.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[52] at parallelize at <console>:29\n",
       "res31: Array[Int] = Array(12, 3, 21)\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Array(12,3,21,2,34))\n",
    "rdd1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ver ejemplo Listing 3.40-3.43 del libro Beginning Apache Spark 2_ With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library (2018, Apress)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### takeSample(withReplacement, n, [seed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of this action is similar to the behavior of the sample transformation. The main difference is this action returns an array of sampled rows to the driver program. The same caution for the collect action is applicable here in terms of the large number of returned rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### takeOrdered(n, [ordering])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This action returns n rows in a certain order. The default ordering for this action is the natural ordering. If the rows are integers, then the default ordering is ascending. If you need to return n rows with the values in descending order, then you specify the reverse ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[53] at parallelize at <console>:29\n",
       "res32: Array[Int] = Array(1, 2, 3, 4)\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,9,10), 2)\n",
    "rdd1.takeOrdered(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Array[Int] = Array(10, 9, 7, 6)\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.takeOrdered(4)(Ordering[Int].reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top(n, [ordering])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good use case for using this action is for figure out the top k (largest) rows in an RDD as defined by the implicit ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[56] at parallelize at <console>:29\n",
       "res34: Array[Int] = Array(67, 45, 33)\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(1,2,33,-1,45,67,-111), 2)\n",
    "rdd1.top(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saveAsTextFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[64] at parallelize at <console>:29\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Array(12,54,32,45,6,2,3,4,56,7,45,3,2,1), 3)\n",
    "rdd1.saveAsTextFile(\"./save_as_text_RDD.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Key/Value Pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,Spark)\n",
      "(2,is)\n",
      "(2,an)\n",
      "(7,amazing)\n",
      "(5,piece)\n",
      "(2,of)\n",
      "(10,technology)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[66] at parallelize at <console>:27\n",
       "pairRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[67] at map at <console>:28\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"Spark\",\"is\",\"an\", \"amazing\", \"piece\",\"of\",\"technology\"))\n",
    "val pairRDD = rdd.map(w => (w.length,w))\n",
    "pairRDD.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key/Value Pair RDD Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key/value pair RDD has additional transformations that are designed to operate on keys. See **Table 3-4** *Common Transformations for Pair RDD*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key/Value Pair RDD Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See **Table 3-5** *Actions for Pair RDD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
