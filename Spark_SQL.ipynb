{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>Spark SQL</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the RDD APIs, the DataFrame APIs are classified into two\n",
    "buckets: transformations and actions. The evaluation semantics are identical in RDDs.\n",
    "Transformations are lazily evaluated, and actions are eagerly evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to create a DataFrame; one common thing among them is the need to provide a schema, either implicitly or explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames from RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.80:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1586532218165)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\n",
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[1] at map at <console>:26\n",
       "kvDF: org.apache.spark.sql.DataFrame = [key: int, value: int]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "val rdd = spark.sparkContext.parallelize(1 to 10).map(x => (x, Random.nextInt(100)* x))\n",
    "val kvDF = rdd.toDF(\"key\",\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the Schema and Showing the Data of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = false)\n",
      " |-- value: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kvDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  1|   60|\n",
      "|  2|    8|\n",
      "|  3|  105|\n",
      "|  4|  160|\n",
      "|  5|  335|\n",
      "|  6|  168|\n",
      "|  7|  371|\n",
      "|  8|  544|\n",
      "|  9|  441|\n",
      "| 10|  280|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kvDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a DataFrame from an RDD with a Schema Created Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "peopleRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[6] at parallelize at <console>:29\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,true), StructField(name,StringType,true), StructField(age,LongType,true))\n",
       "peopleDF: org.apache.spark.sql.DataFrame = [id: bigint, name: string ... 1 more field]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val peopleRDD = sc.parallelize(Array(Row(1L, \"John Doe\", 30L), Row(2L, \"Mary Jane\", 25L)))\n",
    "\n",
    "val schema = StructType(Array(\n",
    "    StructField(\"id\", LongType, true),\n",
    "    StructField(\"name\", StringType, true),\n",
    "    StructField(\"age\", LongType, true)\n",
    "    ))\n",
    "\n",
    "val peopleDF = spark.createDataFrame(peopleRDD, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each StructField object has three pieces of information: name, type, and whether the value is nullable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+\n",
      "| id|     name|age|\n",
      "+---+---------+---+\n",
      "|  1| John Doe| 30|\n",
      "|  2|Mary Jane| 25|\n",
      "+---+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of each column in a DataFrame is mapped to an internal Spark type, which can be a simple scalar type or a complex type. Table 4-1 describes the available internal Spark data types and associated Scala types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames from a Range of Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SparkSession.range Function to Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1: Unit = ()\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.range(5).toDF(\"num\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5,10).toDF(\"num\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "| 11|\n",
      "| 13|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5,15,2).toDF(\"num\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a Collection Tuple to a DataFrame Using Spark’s toDF\n",
    "Implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies: Seq[(String, String, Long)] = List((Damon, Matt,The Bourne Ultimatum,2007), (Damon, Matt,Good Will Hunting,1997))\n",
       "moviesDF: org.apache.spark.sql.DataFrame = [actor: string, title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies = Seq((\"Damon, Matt\", \"The Bourne Ultimatum\", 2007L),\n",
    "                 (\"Damon, Matt\", \"Good Will Hunting\", 1997L))\n",
    "\n",
    "val moviesDF = movies.toDF(\"actor\", \"title\", \"year\")\n",
    "\n",
    "moviesDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----+\n",
      "|      actor|               title|year|\n",
      "+-----------+--------------------+----+\n",
      "|Damon, Matt|The Bourne Ultimatum|2007|\n",
      "|Damon, Matt|   Good Will Hunting|1997|\n",
      "+-----------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames from Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main classes in Spark SQL for reading and writing data are DataFrameReader and DataFrameWriter, respectively. This section will cover the details of working with the APIs in the DataFrameReader class and the various available options when reading data from a specific data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of the DataFrameReader class is available as the read variable of the\n",
    "SparkSession class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@787d1764\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common pattern for interacting with DataFrameReader is described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(...).option(\"key\", value\").schema(...).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 4-2 describes the three main pieces of information that are used when reading data: format, option, and schema.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the Data Source Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.json(\"<path>\")\n",
    "spark.read.format(\"json\")\n",
    "\n",
    "spark.read.parquet(\"<path>\")\n",
    "spark.read.format(\"parquet\")\n",
    "\n",
    "spark.read.jdbc\n",
    "spark.read.format(\"jdbc\")\n",
    "\n",
    "spark.read.orc(\"<path>\")\n",
    "spark.read.format(\"orc\")\n",
    "\n",
    "spark.read.csv(\"<path>\")\n",
    "spark.read.format(\"csv\")\n",
    "\n",
    "spark.read.text(\"<path>\")\n",
    "spark.read.format(\"text\")\n",
    "\n",
    "// custom data source – fully qualifed package name\n",
    "spark.read.format(\"org.example.mysource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames by Reading Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the README.md File As a Text File from a Spark Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val textFile = spark.read.text(\"README.md\")\n",
    "\n",
    "// show 5 lines and don't truncate\n",
    "textFile.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames by Reading CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV parser in Spark is designed to be flexible such that it can parse a text file using a user-provided delimiter. The comma delimiter just happens to be the default one. This means you can use the CSV format to read tab-separated value text files or other text files with an arbitrary delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Table 4-4. CSV Common Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the header and inferSchema options as true won’t require you to specify a schema. Otherwise, you need to define a schema by hand or programmatically create it and pass it into the schema function. If the inferSchema option is false and no schema is provided, Spark will assume the data type of all the columns to be the string type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading CSV Files with Various Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies: org.apache.spark.sql.DataFrame = [actor: string, title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies = spark.read.option(\"header\",\"true\").csv(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies2: org.apache.spark.sql.DataFrame = [actor: string, title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// now try to infer the schema\n",
    "val movies2 = spark.read.option(\"header\",\"true\")\n",
    ".option(\"inferSchema\",\"true\")\n",
    ".csv(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: string (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "movieSchema: org.apache.spark.sql.types.StructType = StructType(StructField(actor_name,StringType,true), StructField(movie_title,StringType,true), StructField(produced_year,LongType,true))\n",
       "movies3: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// now try to manually provide a schema\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val movieSchema = StructType(Array(\n",
    "    StructField(\"actor_name\", StringType, true),\n",
    "    StructField(\"movie_title\", StringType, true),\n",
    "    StructField(\"produced_year\", LongType, true)))\n",
    "\n",
    "val movies3 = spark.read.option(\"header\",\"true\")\n",
    ".schema(movieSchema)\n",
    ".csv(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.csv\")\n",
    "\n",
    "movies3.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------------+-------------+\n",
      "|actor_name       |movie_title                |produced_year|\n",
      "+-----------------+---------------------------+-------------+\n",
      "|McClure, Marc (I)|Freaky Friday              |2003         |\n",
      "|McClure, Marc (I)|Coach Carter               |2005         |\n",
      "|McClure, Marc (I)|Superman II                |1980         |\n",
      "|McClure, Marc (I)|Apollo 13                  |1995         |\n",
      "|McClure, Marc (I)|Superman                   |1978         |\n",
      "|McClure, Marc (I)|Back to the Future         |1985         |\n",
      "|McClure, Marc (I)|Back to the Future Part III|1990         |\n",
      "|Cooper, Chris (I)|Me, Myself & Irene         |2000         |\n",
      "|Cooper, Chris (I)|October Sky                |1999         |\n",
      "|Cooper, Chris (I)|Capote                     |2005         |\n",
      "|Cooper, Chris (I)|The Bourne Supremacy       |2004         |\n",
      "|Cooper, Chris (I)|The Patriot                |2000         |\n",
      "|Cooper, Chris (I)|The Town                   |2010         |\n",
      "|Cooper, Chris (I)|Seabiscuit                 |2003         |\n",
      "|Cooper, Chris (I)|A Time to Kill             |1996         |\n",
      "|Cooper, Chris (I)|Where the Wild Things Are  |2009         |\n",
      "|Cooper, Chris (I)|The Muppets                |2011         |\n",
      "|Cooper, Chris (I)|American Beauty            |1999         |\n",
      "|Cooper, Chris (I)|Syriana                    |2005         |\n",
      "|Cooper, Chris (I)|The Horse Whisperer        |1998         |\n",
      "+-----------------+---------------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies3.show(20,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a TSV File with the CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies4: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies4 = spark.read.option(\"header\", \"true\")\n",
    ".option(\"sep\", \"\\t\")\n",
    ".schema(movieSchema)\n",
    ".csv(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: string (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies4.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames by Reading JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4-5 describes the common options for the JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various Examples of Reading a JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: string (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies5: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies5 = spark.read.json(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.json\") \n",
    "movies5.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: string (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "movieSchema2: org.apache.spark.sql.types.StructType = StructType(StructField(actor_name,StringType,true), StructField(movie_title,StringType,true), StructField(produced_year,IntegerType,true))\n",
       "movies6: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// specify a schema to override the Spark's inferring schema.\n",
    "// producted_year is specified as integer type\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val movieSchema2 = StructType(Array(\n",
    "    StructField(\"actor_name\", StringType, true),\n",
    "    StructField(\"movie_title\", StringType, true),\n",
    "    StructField(\"produced_year\", IntegerType, true)))\n",
    "\n",
    "val movies6 = spark.read.option(\"inferSchema\",\"true\")\n",
    ".schema(movieSchema2)\n",
    ".json(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.json\")\n",
    "\n",
    "movies6.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing Error and How to Tell Spark to Fail Fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: boolean (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "badMovieSchema: org.apache.spark.sql.types.StructType = StructType(StructField(actor_name,BooleanType,true), StructField(movie_title,StringType,true), StructField(produced_year,IntegerType,true))\n",
       "movies7: org.apache.spark.sql.DataFrame = [actor_name: boolean, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// set data type for actor_name as BooleanType\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val badMovieSchema = StructType(Array(\n",
    "    StructField(\"actor_name\", BooleanType, true),\n",
    "    StructField(\"movie_title\", StringType, true),\n",
    "    StructField(\"produced_year\", IntegerType, true)))\n",
    "\n",
    "val movies7 = spark.read.schema(badMovieSchema)\n",
    ".json(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.json\")\n",
    "\n",
    "movies7.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+\n",
      "|actor_name|movie_title|produced_year|\n",
      "+----------+-----------+-------------+\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "|      null|       null|         null|\n",
      "+----------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: boolean (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies8: org.apache.spark.sql.DataFrame = [actor_name: boolean, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// tell Spark to fail fast when facing a parsing error\n",
    "\n",
    "val movies8 = spark.read.option(\"mode\",\"failFast\")\n",
    ".schema(badMovieSchema)\n",
    ".json(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.json\")\n",
    "\n",
    "movies8.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 28, localhost, executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 28, localhost, executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST.",
      "\tat org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:70)",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:143)",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:143)",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "Caused by: java.lang.RuntimeException: Failed to parse a value for data type boolean (current token: VALUE_STRING).",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:327)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:318)",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeConverter$1$$anonfun$apply$5.applyOrElse(JacksonParser.scala:144)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeConverter$1$$anonfun$apply$5.applyOrElse(JacksonParser.scala:144)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$parseJsonToken(JacksonParser.scala:308)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeConverter$1.apply(JacksonParser.scala:144)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeConverter$1.apply(JacksonParser.scala:144)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:343)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeStructRootConverter$1$$anonfun$apply$2.applyOrElse(JacksonParser.scala:76)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeStructRootConverter$1$$anonfun$apply$2.applyOrElse(JacksonParser.scala:75)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$parseJsonToken(JacksonParser.scala:308)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeStructRootConverter$1.apply(JacksonParser.scala:75)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeStructRootConverter$1.apply(JacksonParser.scala:75)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:399)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:394)",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:394)",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$6.apply(JsonDataSource.scala:139)",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$6.apply(JsonDataSource.scala:139)",
      "\tat org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:62)",
      "\t... 29 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)",
      "  ... 44 elided",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST.",
      "  at org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:70)",
      "  at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:143)",
      "  at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:143)",
      "  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)",
      "  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      "Caused by: java.lang.RuntimeException: Failed to parse a value for data type boolean (current token: VALUE_STRING).",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:327)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:318)",
      "  at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeConverter$1$$anonfun$apply$5.applyOrElse(JacksonParser.scala:144)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeConverter$1$$anonfun$apply$5.applyOrElse(JacksonParser.scala:144)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$parseJsonToken(JacksonParser.scala:308)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeConverter$1.apply(JacksonParser.scala:144)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeConverter$1.apply(JacksonParser.scala:144)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:343)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeStructRootConverter$1$$anonfun$apply$2.applyOrElse(JacksonParser.scala:76)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeStructRootConverter$1$$anonfun$apply$2.applyOrElse(JacksonParser.scala:75)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$parseJsonToken(JacksonParser.scala:308)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeStructRootConverter$1.apply(JacksonParser.scala:75)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$makeStructRootConverter$1.apply(JacksonParser.scala:75)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:399)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:394)",
      "  at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)",
      "  at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:394)",
      "  at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$6.apply(JsonDataSource.scala:139)",
      "  at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$6.apply(JsonDataSource.scala:139)",
      "  at org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:62)",
      "  ... 29 more",
      ""
     ]
    }
   ],
   "source": [
    "// Spark will throw a RuntimeException when executing an action\n",
    "\n",
    "movies8.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames by Reading Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is one of the most popular open source columnar storage formats in the Hadoop ecosystem, and it was created at Twitter. Its popularity is because it is a self-describing data format and it stores data in a highly compact structure by leveraging compressions. The columnar storage format is designed to work well with a data analytics workload where only a small subset of the columns are used during the data analysis. **Parquet stores the data of each column in a separate file; therefore, columns that are not needed in a data analysis wouldn’t have to be unnecessarily read in.** It is quite flexible when it comes to supporting a complex data type with a nested structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark works extremely well with the Parquet file format, and in fact Parquet is the default file format for reading and writing data in Spark. Since **Parquet files are selfcontained, meaning the schema is stored inside the Parquet data file**, it is easy to work with Parquet in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a Parquet File in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: string (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies9: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Parquet is the default format, so we don't need to specify the format when reading\n",
    "\n",
    "val movies9 = spark.read.load(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.parquet\")\n",
    "\n",
    "movies9.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: string (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies10: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// If we want to be more explicit, we can specify the path to the parquet function\n",
    "\n",
    "val movies10 = spark.read.parquet(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.parquet\")\n",
    "\n",
    "movies10.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames by Reading ORC Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Row Columnar (ORC) is another popular open source self-describing columnar storage format in the Hadoop ecosystem. It was created by a company called Cloudera as part of the initiative to massively speed up Hive. It is quite similar to Parquet in terms of efficiency and speed and was designed for analytics workloads. Working with ORC files is just as easy as working with Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an ORC File in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: string (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies11: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies11 = spark.read.orc(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.orc\")\n",
    "\n",
    "movies11.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------+\n",
      "|actor_name       |movie_title       |produced_year|\n",
      "+-----------------+------------------+-------------+\n",
      "|McClure, Marc (I)|Coach Carter      |2005         |\n",
      "|McClure, Marc (I)|Superman II       |1980         |\n",
      "|McClure, Marc (I)|Apollo 13         |1995         |\n",
      "|McClure, Marc (I)|Superman          |1978         |\n",
      "|McClure, Marc (I)|Back to the Future|1985         |\n",
      "+-----------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies11.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames from JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JDBC is a standard application API for reading data from and writing data to a relational database management system. Spark has support for JDBC data sources, which means you can use Spark to read data from and write data to any of the existing RDBMSs such as MySQL, PostgreSQL, Oracle, SQLite, and so on. There are a few important pieces of information you need to provide when working with a JDBC data source: a JDBC driver for your RDBMS, a connection URL, authentication information, and a table name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Listing 4.18, 4.19\n",
    "\n",
    "See Table 4-6\n",
    "\n",
    "See Listing 4.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Structured Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the RDD operations, the structured operations are designed to be more relational, meaning these operations mirror the kind of expressions you can do with SQL, such as projection, filtering, transforming, joining, and so on. \n",
    "\n",
    "**Similar to RDD operations, the structured operations are divided into two categories: transformation and action.** The semantics of the structured transformations and actions are identical to the ones in RDDs. In other words, **structured transformations are lazily evaluated, and structured actions are eagerly evaluated.**\n",
    "\n",
    "Structured operations are sometimes described as a *domain-specific language* (DSL) for distributed data manipulation. A DSL is a computer language specialized for a particular application domain. In this case, the application domain is the distributed data manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See Table 4-7. Commonly Used DataFrame Structured Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Table 4-8. Different Ways of Referring to a Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Ways of Referring to a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.80:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1586643159662)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "kvDF: org.apache.spark.sql.DataFrame = [key: int, value: int]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val kvDF = Seq((1,2),(2,3)).toDF(\"key\",\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[String] = Array(key, value)\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// to display column names in a DataFrame, we can call the columns function\n",
    "kvDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvDF.select(\"key\")\n",
    "\n",
    "kvDF.select(col(\"key\"))\n",
    "\n",
    "kvDF.select(column(\"key\"))\n",
    "\n",
    "kvDF.select($\"key\")\n",
    "\n",
    "kvDF.select('key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|key|(key > 1)|\n",
      "+---+---------+\n",
      "|  1|    false|\n",
      "|  2|     true|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// using the col function of DataFrame\n",
    "\n",
    "kvDF.select(kvDF.col(\"key\"))\n",
    "kvDF.select('key, 'key > 1).show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Structured Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies = spark.read.parquet(\"./beginning-apache-spark-2-master/chapter4/data/movies/movies.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Variations of the select Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n",
      "|       movie_title|produced_year|\n",
      "+------------------+-------------+\n",
      "|      Coach Carter|         2005|\n",
      "|       Superman II|         1980|\n",
      "|         Apollo 13|         1995|\n",
      "|          Superman|         1978|\n",
      "|Back to the Future|         1985|\n",
      "+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.select(\"movie_title\",\"produced_year\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|       movie_title|produced_decade|\n",
      "+------------------+---------------+\n",
      "|      Coach Carter|           2000|\n",
      "|       Superman II|           1980|\n",
      "|         Apollo 13|           1990|\n",
      "|          Superman|           1970|\n",
      "|Back to the Future|           1980|\n",
      "+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// using a column expression to transform year to decade\n",
    "\n",
    "movies.select('movie_title,('produced_year - ('produced_year % 10)).\n",
    "as(\"produced_decade\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selectExpr(expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation is a variant of the select transformation. The one big difference is that it accepts one or more SQL expressions, rather than columns. However, both are essentially performing the same projection task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the decade Column to the movies DataFrame Using a SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------+------+\n",
      "|       actor_name|       movie_title|produced_year|decade|\n",
      "+-----------------+------------------+-------------+------+\n",
      "|McClure, Marc (I)|      Coach Carter|         2005|  2000|\n",
      "|McClure, Marc (I)|       Superman II|         1980|  1980|\n",
      "|McClure, Marc (I)|         Apollo 13|         1995|  1990|\n",
      "|McClure, Marc (I)|          Superman|         1978|  1970|\n",
      "|McClure, Marc (I)|Back to the Future|         1985|  1980|\n",
      "+-----------------+------------------+-------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.selectExpr(\"*\",\"(produced_year - (produced_year % 10)) as decade\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a SQL Expression and Built-in Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|movies|actors|\n",
      "+------+------+\n",
      "|  1409|  6527|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.selectExpr(\"count(distinct(movie_title)) as movies\",\"count(distinct(actor_name)) as actors\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter(condition), where(condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Rows with Logical Comparison Functions in the Column Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|         actor_name|         movie_title|produced_year|\n",
      "+-------------------+--------------------+-------------+\n",
      "|  McClure, Marc (I)|         Superman II|         1980|\n",
      "|  McClure, Marc (I)|           Apollo 13|         1995|\n",
      "|  McClure, Marc (I)|            Superman|         1978|\n",
      "|  McClure, Marc (I)|  Back to the Future|         1985|\n",
      "|  McClure, Marc (I)|Back to the Futur...|         1990|\n",
      "|  Cooper, Chris (I)|         October Sky|         1999|\n",
      "|  Cooper, Chris (I)|      A Time to Kill|         1996|\n",
      "|  Cooper, Chris (I)|     American Beauty|         1999|\n",
      "|  Cooper, Chris (I)| The Horse Whisperer|         1998|\n",
      "|Knight, Shirley (I)|  As Good as It Gets|         1997|\n",
      "+-------------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.filter(col(\"produced_year\") < 2000).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.filter('produced_year < 2000)\n",
    "movies.where('produced_year > 2000)\n",
    "\n",
    "movies.filter('produced_year >= 2000)\n",
    "movies.where('produced_year >= 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Equality comparison require 3 equal signs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------+\n",
      "|       actor_name|         movie_title|produced_year|\n",
      "+-----------------+--------------------+-------------+\n",
      "|Cooper, Chris (I)|  Me, Myself & Irene|         2000|\n",
      "|Cooper, Chris (I)|         The Patriot|         2000|\n",
      "|  Jolie, Angelina|Gone in Sixty Sec...|         2000|\n",
      "|   Yip, Françoise|      Romeo Must Die|         2000|\n",
      "|   Danner, Blythe|    Meet the Parents|         2000|\n",
      "+-----------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.filter('produced_year === 2000).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------+\n",
      "|       actor_name|         movie_title|produced_year|\n",
      "+-----------------+--------------------+-------------+\n",
      "|Cooper, Chris (I)|  Me, Myself & Irene|         2000|\n",
      "|Cooper, Chris (I)|         The Patriot|         2000|\n",
      "|  Jolie, Angelina|Gone in Sixty Sec...|         2000|\n",
      "|   Yip, Françoise|      Romeo Must Die|         2000|\n",
      "|   Danner, Blythe|    Meet the Parents|         2000|\n",
      "+-----------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.filter(col(\"produced_year\") === 2000).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Inequality comparison uses an interesting looking operator =!="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|         movie_title|produced_year|\n",
      "+--------------------+-------------+\n",
      "|        Coach Carter|         2005|\n",
      "|         Superman II|         1980|\n",
      "|           Apollo 13|         1995|\n",
      "|            Superman|         1978|\n",
      "|  Back to the Future|         1985|\n",
      "|Back to the Futur...|         1990|\n",
      "|         October Sky|         1999|\n",
      "|              Capote|         2005|\n",
      "|The Bourne Supremacy|         2004|\n",
      "|            The Town|         2010|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.select(\"movie_title\", \"produced_year\").filter('produced_year =!= 2000).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|         movie_title|produced_year|\n",
      "+--------------------+-------------+\n",
      "|        Coach Carter|         2005|\n",
      "|         Superman II|         1980|\n",
      "|           Apollo 13|         1995|\n",
      "|            Superman|         1978|\n",
      "|  Back to the Future|         1985|\n",
      "|Back to the Futur...|         1990|\n",
      "|         October Sky|         1999|\n",
      "|              Capote|         2005|\n",
      "|The Bourne Supremacy|         2004|\n",
      "|            The Town|         2010|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Otra forma\n",
    "movies.select(\"movie_title\", \"produced_year\").filter(col(\"produced_year\") =!= 2000).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine one or more comparison expressions, we will use either the OR and AND expression operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|     actor_name|movie_title|produced_year|\n",
      "+---------------+-----------+-------------+\n",
      "|Jolie, Angelina|       Salt|         2010|\n",
      "| Cueto, Esteban|        xXx|         2002|\n",
      "|  Butters, Mike|        Saw|         2004|\n",
      "| Franko, Victor|         21|         2008|\n",
      "|  Ogbonna, Chuk|       Salt|         2010|\n",
      "+---------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.filter('produced_year > 2000 && length('movie_title) < 5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way of accomplishing the same result is by calling the filter function two times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|     actor_name|movie_title|produced_year|\n",
      "+---------------+-----------+-------------+\n",
      "|Jolie, Angelina|       Salt|         2010|\n",
      "| Cueto, Esteban|        xXx|         2002|\n",
      "|  Butters, Mike|        Saw|         2004|\n",
      "| Franko, Victor|         21|         2008|\n",
      "|  Ogbonna, Chuk|       Salt|         2010|\n",
      "+---------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.filter('produced_year > 2000).filter(length('movie_title) < 5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct, dropDuplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two transformations have identical behavior. However, dropDuplicates allows you to control which columns should be used in deduplication logic. If none is specified, the deduplication logic will use all the columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|movies|\n",
      "+------+\n",
      "|  1409|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.select(\"movie_title\").distinct.selectExpr(\"count(movie_title) as movies\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort(columns), orderBy(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these transformations have the same semantics. The orderBy transformation is more relational than the other one. By default, the sorting is in ascending order, and it is fairly easy to change it to descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------+-------------+\n",
      "|movie_title                        |title_length|produced_year|\n",
      "+-----------------------------------+------------+-------------+\n",
      "|Poseidon                           |8           |2006         |\n",
      "|Ironiya sudby. Prodolzhenie        |27          |2007         |\n",
      "|The Last Airbender                 |18          |2010         |\n",
      "|Failure to Launch                  |17          |2006         |\n",
      "|Le fabuleux destin d'Amélie Poulain|35          |2001         |\n",
      "+-----------------------------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movieTitles: org.apache.spark.sql.DataFrame = [movie_title: string, title_length: int ... 1 more field]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movieTitles = movies.dropDuplicates(\"movie_title\")\n",
    ".selectExpr(\"movie_title\", \"length(movie_title) as title_length\", \"produced_year\")\n",
    "\n",
    "movieTitles.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------+\n",
      "|movie_title|title_length|produced_year|\n",
      "+-----------+------------+-------------+\n",
      "|         RV|           2|         2006|\n",
      "|         12|           2|         2007|\n",
      "|         Up|           2|         2009|\n",
      "|         X2|           2|         2003|\n",
      "|         21|           2|         2008|\n",
      "+-----------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieTitles.sort('title_length).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+------------+-------------+\n",
      "|movie_title                                                                        |title_length|produced_year|\n",
      "+-----------------------------------------------------------------------------------+------------+-------------+\n",
      "|Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan|83          |2006         |\n",
      "|The Chronicles of Narnia: The Lion, the Witch and the Wardrobe                     |62          |2005         |\n",
      "|Hannah Montana & Miley Cyrus: Best of Both Worlds Concert                          |57          |2008         |\n",
      "|The Chronicles of Narnia: The Voyage of the Dawn Treader                           |56          |2010         |\n",
      "|Istoriya pro Richarda, milorda i prekrasnuyu Zhar-ptitsu                           |56          |1997         |\n",
      "+-----------------------------------------------------------------------------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieTitles.sort('title_length.desc).show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by two columns in different orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+------------+-------------+\n",
      "|movie_title                                                                        |title_length|produced_year|\n",
      "+-----------------------------------------------------------------------------------+------------+-------------+\n",
      "|Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan|83          |2006         |\n",
      "|The Chronicles of Narnia: The Lion, the Witch and the Wardrobe                     |62          |2005         |\n",
      "|Hannah Montana & Miley Cyrus: Best of Both Worlds Concert                          |57          |2008         |\n",
      "|Istoriya pro Richarda, milorda i prekrasnuyu Zhar-ptitsu                           |56          |1997         |\n",
      "|The Chronicles of Narnia: The Voyage of the Dawn Treader                           |56          |2010         |\n",
      "|Pirates of the Caribbean: The Curse of the Black Pearl                             |54          |2003         |\n",
      "+-----------------------------------------------------------------------------------+------------+-------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieTitles.orderBy('title_length.desc, 'produced_year).show(6, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limit(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation returns a new DataFrame by taking the first n rows. This transformation is commonly used after the sorting is done to figure out the top n or bottom n rows based on the sorting order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actorNameDF: org.apache.spark.sql.DataFrame = [actor_name: string, length: int]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val actorNameDF = movies.select(\"actor_name\").distinct.selectExpr(\"*\", \"length(actor_name) as length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|          actor_name|length|\n",
      "+--------------------+------+\n",
      "|Badalamenti II, P...|    28|\n",
      "|Driscoll, Timothy...|    28|\n",
      "|Phillips, Christo...|    27|\n",
      "|Marshall-Fricker,...|    27|\n",
      "|Shepard, Maridean...|    27|\n",
      "|Martino, Nicholas...|    27|\n",
      "|Pahlavi, Shah Moh...|    27|\n",
      "|Lawrence, Mark Ch...|    26|\n",
      "|Van de Kamp Bucha...|    26|\n",
      "|Lough Haggquist, ...|    26|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actorNameDF.sort(col(\"length\").desc).limit(10).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|          actor_name|length|\n",
      "+--------------------+------+\n",
      "|Badalamenti II, P...|    28|\n",
      "|Driscoll, Timothy...|    28|\n",
      "|Phillips, Christo...|    27|\n",
      "|Marshall-Fricker,...|    27|\n",
      "|Shepard, Maridean...|    27|\n",
      "|Martino, Nicholas...|    27|\n",
      "|Pahlavi, Shah Moh...|    27|\n",
      "|Lawrence, Mark Ch...|    26|\n",
      "|Van de Kamp Bucha...|    26|\n",
      "|Lough Haggquist, ...|    26|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Otra forma\n",
    "actorNameDF.orderBy('length.desc).limit(10).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union(otherDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You learned earlier that DataFrames are immutable. So if there is a need to add more rows to an existing DataFrame, then the union transformation is useful for that purpose as well as for combining rows from two DataFrames. **This transformation requires both DataFrames to have the same schema, meaning both column names and their order must exactly match.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** We want to add a missing actor to movie with title as \"12\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------+\n",
      "|          actor_name|movie_title|produced_year|\n",
      "+--------------------+-----------+-------------+\n",
      "|    Efremov, Mikhail|         12|         2007|\n",
      "|     Stoyanov, Yuriy|         12|         2007|\n",
      "|     Gazarov, Sergey|         12|         2007|\n",
      "|Verzhbitskiy, Viktor|         12|         2007|\n",
      "+--------------------+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "shortNameMovieDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val shortNameMovieDF = movies.where('movie_title === \"12\")\n",
    "shortNameMovieDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame with one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "actor: Seq[org.apache.spark.sql.Row] = List([Brychta, Edita,12,2007])\n",
       "actorRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[138] at parallelize at <console>:34\n",
       "actorDF: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val actor = Seq(Row(\"Brychta, Edita\", \"12\", 2007L))\n",
    "val actorRDD = sc.parallelize(actor)\n",
    "val actorDF = spark.createDataFrame(actorRDD, shortNameMovieDF.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------+\n",
      "|    actor_name|movie_title|produced_year|\n",
      "+--------------+-----------+-------------+\n",
      "|Brychta, Edita|         12|         2007|\n",
      "+--------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actorDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------+\n",
      "|          actor_name|movie_title|produced_year|\n",
      "+--------------------+-----------+-------------+\n",
      "|    Efremov, Mikhail|         12|         2007|\n",
      "|     Stoyanov, Yuriy|         12|         2007|\n",
      "|     Gazarov, Sergey|         12|         2007|\n",
      "|Verzhbitskiy, Viktor|         12|         2007|\n",
      "|      Brychta, Edita|         12|         2007|\n",
      "+--------------------+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "completeDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val completeDF = shortNameMovieDF.union(actorDF)\n",
    "completeDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumn(colName, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation is used to add a new column to a DataFrame. It requires two input parameters: a column name and a value in the form of a column expression. You can accomplish pretty much the same goal by using the selectExpr transformation. However, if the given column name matches one of the existing ones, then that column is replaced with the given column expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a new column based on a certain column expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------+------+\n",
      "|actor_name       |movie_title       |produced_year|decade|\n",
      "+-----------------+------------------+-------------+------+\n",
      "|McClure, Marc (I)|Coach Carter      |2005         |2000  |\n",
      "|McClure, Marc (I)|Superman II       |1980         |1980  |\n",
      "|McClure, Marc (I)|Apollo 13         |1995         |1990  |\n",
      "|McClure, Marc (I)|Superman          |1978         |1970  |\n",
      "|McClure, Marc (I)|Back to the Future|1985         |1980  |\n",
      "+-----------------+------------------+-------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.withColumn(\"decade\", ('produced_year - ('produced_year % 10))).show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------+------+\n",
      "|actor_name       |movie_title       |produced_year|decade|\n",
      "+-----------------+------------------+-------------+------+\n",
      "|McClure, Marc (I)|Coach Carter      |2005         |2000  |\n",
      "|McClure, Marc (I)|Superman II       |1980         |1980  |\n",
      "|McClure, Marc (I)|Apollo 13         |1995         |1990  |\n",
      "|McClure, Marc (I)|Superman          |1978         |1970  |\n",
      "|McClure, Marc (I)|Back to the Future|1985         |1980  |\n",
      "+-----------------+------------------+-------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.withColumn(\"decade\", (col(\"produced_year\") - (col(\"produced_year\") % 10))).show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now replace the produced_year with new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------+\n",
      "|actor_name       |movie_title       |produced_year|\n",
      "+-----------------+------------------+-------------+\n",
      "|McClure, Marc (I)|Coach Carter      |2000         |\n",
      "|McClure, Marc (I)|Superman II       |1980         |\n",
      "|McClure, Marc (I)|Apollo 13         |1990         |\n",
      "|McClure, Marc (I)|Superman          |1970         |\n",
      "|McClure, Marc (I)|Back to the Future|1980         |\n",
      "+-----------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.withColumn(\"produced_year\", ('produced_year - ('produced_year % 10))).show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumnRenamed(existingColName, newColName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See page 123 of PDF for understain this topic in deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the withColumnRenamed Transformation to Rename Some of the Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+----+\n",
      "|            actor|             title|year|\n",
      "+-----------------+------------------+----+\n",
      "|McClure, Marc (I)|      Coach Carter|2005|\n",
      "|McClure, Marc (I)|       Superman II|1980|\n",
      "|McClure, Marc (I)|         Apollo 13|1995|\n",
      "|McClure, Marc (I)|          Superman|1978|\n",
      "|McClure, Marc (I)|Back to the Future|1985|\n",
      "+-----------------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.withColumnRenamed(\"actor_name\", \"actor\")\n",
    ".withColumnRenamed(\"movie_title\", \"title\")\n",
    ".withColumnRenamed(\"produced_year\", \"year\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop(columnName1, columnName2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation simply drops the specified columns from the DataFrame. You can specify one or more column names to drop, but only the ones that exist in the schema\n",
    "will be dropped and the ones that don’t will be silently ignored. You can use the select\n",
    "transformation to drop columns by projecting only the columns that you want to keep.\n",
    "In the case that a DataFrame has 100 columns and you want to drop a few, then this\n",
    "transformation is more convenient to use than the select transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Two Columns: One Exists and the Other One Doesn’t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.drop(\"actor_name\", \"me\").printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the previous example, the second column, me, doesn’t exist in\n",
    "the schema, so the drop transformation simply ignores it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample(fraction), sample(fraction, seed), sample(fraction, seed, withReplacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation returns a randomly selected set of rows from the DataFrame. The\n",
    "number of the returned rows will be approximately equal to the specified fraction, which\n",
    "represents a percentage, and the value has to be between 0 and 1. The seed is used to\n",
    "seed the random number generator, which is used to generate a row number to include\n",
    "in the result. If a seed is not specified, then a randomly generated value is used. The\n",
    "withReplacement option is used to determine whether a randomly selected row will be\n",
    "placed back into the selection pool. In other words, when withReplacement is true, a\n",
    "particular selected row has the potential to be selected more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------+\n",
      "|       actor_name|         movie_title|produced_year|\n",
      "+-----------------+--------------------+-------------+\n",
      "|List, Peyton (II)|The Sorcerer's Ap...|         2010|\n",
      "|Kehoe, Michael G.|       Jerry Maguire|         1996|\n",
      "|   Hardrict, Cory|   Never Been Kissed|         1999|\n",
      "+-----------------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// sample with no replacement and a fraction\n",
    "movies.sample(false, 0.0003).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-------------+\n",
      "|          actor_name|   movie_title|produced_year|\n",
      "+--------------------+--------------+-------------+\n",
      "|Panzarella, Russ (V)|Public Enemies|         2009|\n",
      "|        Reed, Tanoai|     Daredevil|         2003|\n",
      "|        Moyo, Masasa|  Spider-Man 3|         2007|\n",
      "+--------------------+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// sample with replacement, a fraction and a seed\n",
    "movies.sample(true, 0.0003, 123456).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomSplit(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation is commonly used during the process of preparing the data to train\n",
    "machine learning models. Unlike the previous transformations, this one returns one\n",
    "or more DataFrames. The number of DataFrames it returns is based on the number of\n",
    "weights you specify. If the provided set of weights don’t add up to 1, then they will be\n",
    "normalized accordingly to add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smallerMovieDFs: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([actor_name: string, movie_title: string ... 1 more field], [actor_name: string, movie_title: string ... 1 more field], [actor_name: string, movie_title: string ... 1 more field])\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val smallerMovieDFs = movies.randomSplit(Array(0.6, 0.3, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: Long = 18903\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallerMovieDFs(0).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res48: Long = 9328\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallerMovieDFs(1).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Missing or Bad Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides a dedicated class called DataFrameNaFunctions to help in\n",
    "dealing with this inconvenient issue. An instance of DataFrameNaFunctions is available\n",
    "as the an member variable inside the DataFrame class. There are three common ways\n",
    "of dealing with missing or bad data. The first way is to drop the rows that have missing\n",
    "values in a one or more columns. The second way is to fill those missing values with\n",
    "user-provided values. The third way is to replace the bad data with something that you\n",
    "know how to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Rows with Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+\n",
      "|actor_name|  movie_title|produced_year|\n",
      "+----------+-------------+-------------+\n",
      "|      null|         null|         null|\n",
      "|      null|         null|         2018|\n",
      "|  John Doe|Awesome Movie|         null|\n",
      "|      null|Awesome Movie|         2018|\n",
      "| Mary Jane|         null|         2018|\n",
      "+----------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "badMovies: Seq[org.apache.spark.sql.Row] = List([null,null,null], [null,null,2018], [John Doe,Awesome Movie,null], [null,Awesome Movie,2018], [Mary Jane,null,2018])\n",
       "badMoviesRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[191] at parallelize at <console>:40\n",
       "badMoviesDF: org.apache.spark.sql.DataFrame = [actor_name: string, movie_title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// first create a DataFrame with missing values in one or more columns\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val badMovies = Seq(Row(null, null, null),\n",
    "                    Row(null, null, 2018L),\n",
    "                    Row(\"John Doe\", \"Awesome Movie\", null),\n",
    "                    Row(null, \"Awesome Movie\", 2018L),\n",
    "                    Row(\"Mary Jane\", null, 2018L))\n",
    "\n",
    "val badMoviesRDD = spark.sparkContext.parallelize(badMovies)\n",
    "\n",
    "val badMoviesDF = spark.createDataFrame(badMoviesRDD, movies.schema)\n",
    "\n",
    "badMoviesDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+\n",
      "|actor_name|movie_title|produced_year|\n",
      "+----------+-----------+-------------+\n",
      "+----------+-----------+-------------+\n",
      "\n",
      "+----------+-----------+-------------+\n",
      "|actor_name|movie_title|produced_year|\n",
      "+----------+-----------+-------------+\n",
      "+----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// dropping rows that have missing data in any column\n",
    "// both of the lines below will achieve the same purpose\n",
    "\n",
    "badMoviesDF.na.drop().show\n",
    "badMoviesDF.na.drop(\"any\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+\n",
      "|actor_name|  movie_title|produced_year|\n",
      "+----------+-------------+-------------+\n",
      "|      null|         null|         2018|\n",
      "|  John Doe|Awesome Movie|         null|\n",
      "|      null|Awesome Movie|         2018|\n",
      "| Mary Jane|         null|         2018|\n",
      "+----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// drop rows that have missing data in every single column\n",
    "badMoviesDF.na.drop(\"all\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+\n",
      "|actor_name|  movie_title|produced_year|\n",
      "+----------+-------------+-------------+\n",
      "|  John Doe|Awesome Movie|         null|\n",
      "| Mary Jane|         null|         2018|\n",
      "+----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// drops rows when column actor_name has missing data\n",
    "badMoviesDF.na.drop(Array(\"actor_name\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### describe(columnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to have a general sense of the basic statistics of the data you\n",
    "are working with. The basic statistics this transformation can compute for string and\n",
    "numeric columns are count, mean, standard deviation, minimum, and maximum.\n",
    "You can pick and choose which string or numeric columns to compute the statistics for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|     produced_year|\n",
      "+-------+------------------+\n",
      "|  count|             31392|\n",
      "|   mean|2002.7964449541284|\n",
      "| stddev| 6.377236851493877|\n",
      "|    min|              1961|\n",
      "|    max|              2012|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.describe(\"produced_year\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Structured Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Table 4-9. Commonly Used Structured Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See page 130 PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data Out to Storage Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Interacting Pattern with DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.write.format(...).mode(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// write data out as CVS format, but using a '#' as delimiter\n",
    "movies.write.format(\"csv\").option(\"sep\", \"#\").save(\"/tmp/output/csv\")\n",
    "\n",
    "// write data out using overwrite save mode\n",
    "movies.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"#\").save\n",
    "(\"/tmp/output/csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the Number of Partitions in a DataFrame to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val singlePartitionDF = movies.coalesce(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partitions:** The idea of writing data out using partitioning and bucketing is borrowed from\n",
    "the Apache Hive user community. As a general rule of thumb, the partition by column\n",
    "should have low cardinality. In the movies DataFrame, the produced_year column is\n",
    "a good candidate for the partition by column. Let’s say you are going to write out the\n",
    "movies DataFrame with partitioning by the produced_year column. DataFrameWriter\n",
    "will write out all the movies with the same produced_year into a single directory. The\n",
    "number of directories in the output folder will correspond to the number of years in the\n",
    "movies DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the movies DataFrame Using the Parquet Format and\n",
    "Partition by the produced_year Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.write.partitionBy(\"produced_year\").save(\"/tmp/output/movies\")\n",
    "\n",
    "// the /tmp/output/movies directory will contain the following subdirectories produced_year=1961 to produced_year=2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
